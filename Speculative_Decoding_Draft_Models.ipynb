{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrzaqjXS9zNcF2iUmzq+/q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheRadDani/Speculative-LLM-Decoding-Draft-Models/blob/main/Speculative_Decoding_Draft_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RVyb3JJ32bYu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.nn.functional as F # For softmax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_model_name = \"EleutherAI/gpt-neo-125m\"\n",
        "draft_model_name = \"distilbert/distilgpt2\""
      ],
      "metadata": {
        "id": "ay-gsG1H2rhd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(target_model_name)"
      ],
      "metadata": {
        "id": "xaXPRHgR28Rk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "CANONICAL_VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "print(f\"Running on device: {device}\")\n",
        "print(f\"Target Model: {target_model_name}\")\n",
        "print(f\"Draft Model: {draft_model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEF_IpnE3eS3",
        "outputId": "99ef3d31-d078-4e69-e9e7-51da377951de"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cpu\n",
            "Target Model: EleutherAI/gpt-neo-125m\n",
            "Draft Model: distilbert/distilgpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Final tokenizer vocab size (len): {len(tokenizer)}\")\n",
        "print(f\"Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n",
        "print(f\"Tokenizer eos_token_id: {tokenizer.eos_token_id}\")\n",
        "print(f\"CANONICAL_VOCAB_SIZE for validation: {CANONICAL_VOCAB_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88vrbsTo_shW",
        "outputId": "51d1ff9a-8858-4769-8289-a5300fc72a9a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final tokenizer vocab size (len): 50257\n",
            "Tokenizer pad_token_id: 50256\n",
            "Tokenizer eos_token_id: 50256\n",
            "CANONICAL_VOCAB_SIZE for validation: 50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.pad_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e4hBlmv3pB5",
        "outputId": "599b8ae1-2894-45a8-a1ac-4074648ac111"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    CANONICAL_VOCAB_SIZE = len(tokenizer) # Update canonical size"
      ],
      "metadata": {
        "id": "knLzeK2e3iFi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For target model\n",
        "\n",
        "target_config = AutoConfig.from_pretrained(target_model_name)\n",
        "if target_config.vocab_size < CANONICAL_VOCAB_SIZE:\n",
        "    target_config.vocab_size = CANONICAL_VOCAB_SIZE # Update config before loading model\n",
        "    print(f\"Adjusting target model config vocab_size to {CANONICAL_VOCAB_SIZE}\")\n",
        "target_model = AutoModelForCausalLM.from_pretrained(target_model_name, config=target_config)\n",
        "# Explicitly resize embeddings in case the model's loaded vocab size is still smaller\n",
        "if target_model.get_input_embeddings().num_embeddings < CANONICAL_VOCAB_SIZE:\n",
        "    target_model.resize_token_embeddings(CANONICAL_VOCAB_SIZE)\n",
        "    print(f\"Resized target model embeddings to {target_model.get_input_embeddings().num_embeddings}\")"
      ],
      "metadata": {
        "id": "a8spNtYLAv9a"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For draft model\n",
        "draft_config = AutoConfig.from_pretrained(draft_model_name)\n",
        "if draft_config.vocab_size < CANONICAL_VOCAB_SIZE:\n",
        "    draft_config.vocab_size = CANONICAL_VOCAB_SIZE # Update config before loading model\n",
        "    print(f\"Adjusting draft model config vocab_size to {CANONICAL_VOCAB_SIZE}\")\n",
        "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, config=draft_config)\n",
        "# Explicitly resize embeddings in case the model's loaded vocab size is still smaller\n",
        "if draft_model.get_input_embeddings().num_embeddings < CANONICAL_VOCAB_SIZE:\n",
        "    draft_model.resize_token_embeddings(CANONICAL_VOCAB_SIZE)\n",
        "    print(f\"Resized draft model embeddings to {draft_model.get_input_embeddings().num_embeddings}\")\n",
        "\n",
        "target_model.to(device)\n",
        "draft_model.to(device)\n",
        "\n",
        "print(f\"Final target model vocab size (from model.config): {target_model.config.vocab_size}\")\n",
        "print(f\"Final draft model vocab size (from model.config): {draft_model.config.vocab_size}\")\n",
        "print(f\"Target model embedding layer size: {target_model.get_input_embeddings().num_embeddings}\")\n",
        "print(f\"Draft model embedding layer size: {draft_model.get_input_embeddings().num_embeddings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IIc1DX0BFh0",
        "outputId": "b31e54a4-74af-40b5-8c3d-f76327452163"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final target model vocab size (from model.config): 50257\n",
            "Final draft model vocab size (from model.config): 50257\n",
            "Target model embedding layer size: 50257\n",
            "Draft model embedding layer size: 50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_token(logits, temperature=1.0, top_k=0, top_p=1.0, model_vocab_size=None):\n",
        "    \"\"\"\n",
        "    Samples the next token from the logits, with optional temperature, Top-K, and Top-P sampling.\n",
        "    Includes robust validation for sampled token IDs.\n",
        "    \"\"\"\n",
        "    if temperature == 0.0: # Greedy decoding\n",
        "        next_token_id = torch.argmax(logits, dim=-1).item()\n",
        "    else:\n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Top-K sampling\n",
        "        if top_k > 0:\n",
        "            top_k_actual = min(top_k, logits.size(-1))\n",
        "            values, _ = torch.topk(logits, top_k_actual)\n",
        "            min_value = values[:, -1].unsqueeze(-1)\n",
        "            logits = torch.where(logits < min_value, torch.full_like(logits, -float('Inf')), logits)\n",
        "\n",
        "        # Top-P (nucleus) sampling\n",
        "        if top_p < 1.0:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "            logits = logits.scatter_(-1, sorted_indices[sorted_indices_to_remove], float('-Inf'))\n",
        "\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Handle cases where probabilities might become all zero after aggressive filtering\n",
        "        probabilities = torch.nan_to_num(probabilities, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        if probabilities.sum().item() == 0.0:\n",
        "            # Fallback to EOS or a safe token if no valid tokens can be sampled\n",
        "            print(\"Warning: All probabilities zero after filtering. Falling back to EOS.\")\n",
        "            return tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0\n",
        "\n",
        "        next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
        "\n",
        "    # --- Robust validation for sampled token ID ---\n",
        "    if model_vocab_size is not None:\n",
        "        if not (0 <= next_token_id < model_vocab_size):\n",
        "            print(f\"!!! CRITICAL ERROR: Sampled token ID {next_token_id} is out of vocabulary range [{0}, {model_vocab_size-1}]\")\n",
        "            print(f\"Logits shape: {logits.shape}\")\n",
        "            print(f\"Probabilities sum: {probabilities.sum().item():.4f}\")\n",
        "            top_probs, top_indices = torch.topk(probabilities, k=min(10, probabilities.size(-1)))\n",
        "            print(f\"Top 10 Probs: {top_probs.tolist()}\")\n",
        "            print(f\"Top 10 Indices: {top_indices.tolist()}\")\n",
        "            raise IndexError(\"Sampled token ID out of model's vocabulary range.\")\n",
        "\n",
        "    return next_token_id"
      ],
      "metadata": {
        "id": "LjSfUkeDH4j8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoregressive_decode_with_sampling(prompt: str, target_model, tokenizer,\n",
        "                                         max_new_tokens: int = 50,\n",
        "                                         temperature: float = 1.0,\n",
        "                                         top_k: int = 0,\n",
        "                                         top_p: float = 1.0):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = []\n",
        "    past_key_values = None\n",
        "\n",
        "    start_time = time.time()\n",
        "    for _ in range(max_new_tokens):\n",
        "        # --- Validate input_ids before passing to model ---\n",
        "        if input_ids.numel() > 0 and (input_ids.min() < 0 or input_ids.max() >= CANONICAL_VOCAB_SIZE):\n",
        "            print(f\"!!! ERROR: AR input_ids min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
        "            print(f\"AR input_ids: {input_ids}\")\n",
        "            raise IndexError(\"AR input_ids contain out-of-range token for target model.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = target_model(input_ids, past_key_values=past_key_values, use_cache=True)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            past_key_values = outputs.past_key_values\n",
        "\n",
        "            next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                                             model_vocab_size=CANONICAL_VOCAB_SIZE)\n",
        "\n",
        "            generated_tokens.append(next_token_id)\n",
        "            input_ids = torch.tensor([[next_token_id]]).to(device)\n",
        "\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    end_time = time.time()\n",
        "    full_sequence = tokenizer.decode(tokenizer.encode(prompt) + generated_tokens, skip_special_tokens=True)\n",
        "    return full_sequence, len(generated_tokens), (end_time - start_time)"
      ],
      "metadata": {
        "id": "8Q1E4pzx4Ulc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Demonstration of Autoregressive Decoding with Sampling ---\n",
        "prompt = \"There is not a way this story has this end in this book named\"\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding (Greedy - Temperature 0.0) ---\")\n",
        "ar_greedy_output, ar_greedy_tokens, ar_greedy_time = autoregressive_decode_with_sampling(\n",
        "    prompt, target_model, tokenizer, max_new_tokens=50, temperature=0.0\n",
        ")\n",
        "print(f\"Output: '{ar_greedy_output}'\")\n",
        "print(f\"Tokens Generated: {ar_greedy_tokens}\")\n",
        "print(f\"Time Taken: {ar_greedy_time:.4f} seconds\")\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding (Temperature 0.7) ---\")\n",
        "ar_temp_output, ar_temp_tokens, ar_temp_time = autoregressive_decode_with_sampling(\n",
        "    prompt, target_model, tokenizer, max_new_tokens=50, temperature=0.7\n",
        ")\n",
        "print(f\"Output: '{ar_temp_output}'\")\n",
        "print(f\"Tokens Generated: {ar_temp_tokens}\")\n",
        "print(f\"Time Taken: {ar_temp_time:.4f} seconds\")\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding (Top-K = 50, Temperature 1.0) ---\")\n",
        "ar_topk_output, ar_topk_tokens, ar_topk_time = autoregressive_decode_with_sampling(\n",
        "    prompt, target_model, tokenizer, max_new_tokens=50, temperature=1.0, top_k=50\n",
        ")\n",
        "print(f\"Output: '{ar_topk_output}'\")\n",
        "print(f\"Tokens Generated: {ar_topk_tokens}\")\n",
        "print(f\"Time Taken: {ar_topk_time:.4f} seconds\")\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding (Top-P = 0.9, Temperature 1.0) ---\")\n",
        "ar_topp_output, ar_topp_tokens, ar_topp_time = autoregressive_decode_with_sampling(\n",
        "    prompt, target_model, tokenizer, max_new_tokens=50, temperature=1.0, top_p=0.9\n",
        ")\n",
        "print(f\"Output: '{ar_topp_output}'\")\n",
        "print(f\"Tokens Generated: {ar_topp_tokens}\")\n",
        "print(f\"Time Taken: {ar_topp_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "_hi6UZMZQSO2",
        "outputId": "2e70235c-1865-4ea7-eff5-0356cb752f48"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Autoregressive Decoding (Greedy - Temperature 0.0) ---\n",
            "Output: 'There is not a way this story has this end in this book named \"The End of the World\" by the author.\n",
            "\n",
            "The end of the world is a story of the end of the world.\n",
            "\n",
            "The end of the world is a story of the end of the world.\n",
            "\n",
            "The end of'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 5.2820 seconds\n",
            "\n",
            "--- Autoregressive Decoding (Temperature 0.7) ---\n",
            "Output: 'There is not a way this story has this end in this book named for it.\n",
            "\n",
            "Star Wars: Episode IX: The Phantom Pain is set in the future and is about to face the end of the era of the Empire. The Phantom Pain is only one of a bunch of things that stand in the way of'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 3.3620 seconds\n",
            "\n",
            "--- Autoregressive Decoding (Top-K = 50, Temperature 1.0) ---\n",
            "Output: 'There is not a way this story has this end in this book named this day, and in this case, no, not tomorrow.\"\n",
            "\n",
            "\"Oh, I see. That's why the name is written in the newspaper.\"\n",
            "\n",
            "\"That's why I think someone must remember you just right. So the time'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 3.8390 seconds\n",
            "\n",
            "--- Autoregressive Decoding (Top-P = 0.9, Temperature 1.0) ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Index tensor must have the same number of dimensions as self tensor",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-53-90511841.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Autoregressive Decoding (Top-P = 0.9, Temperature 1.0) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m ar_topp_output, ar_topp_tokens, ar_topp_time = autoregressive_decode_with_sampling(\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m )\n",
            "\u001b[0;32m/tmp/ipython-input-51-1106816341.py\u001b[0m in \u001b[0;36mautoregressive_decode_with_sampling\u001b[0;34m(prompt, target_model, tokenizer, max_new_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mpast_key_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n\u001b[0m\u001b[1;32m     25\u001b[0m                                              model_vocab_size=CANONICAL_VOCAB_SIZE)\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-50-1998889866.py\u001b[0m in \u001b[0;36msample_next_token\u001b[0;34m(logits, temperature, top_k, top_p, model_vocab_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0msorted_indices_to_remove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted_indices_to_remove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-Inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as self tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def speculative_decode(prompt: str, target_model, draft_model, tokenizer,\n",
        "                       max_new_tokens: int = 50, speculative_lookahead: int = 5,\n",
        "                       temperature: float = 1.0, top_k: int = 0, top_p: float = 1.0): # Added sampling params\n",
        "    \"\"\"\n",
        "    Speculative decoding implementation.\n",
        "    Args:\n",
        "        prompt: Initial input prompt string.\n",
        "        target_model: The larger, accurate language model.\n",
        "        draft_model: The smaller, faster draft model.\n",
        "        tokenizer: Tokenizer for both models.\n",
        "        max_new_tokens: Maximum number of tokens to generate.\n",
        "        speculative_lookahead: Number of tokens the draft model speculates.\n",
        "        temperature: Sampling temperature for target model fallback.\n",
        "        top_k: Top-K sampling parameter for target model fallback.\n",
        "        top_p: Top-P sampling parameter for target model fallback.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = []\n",
        "\n",
        "    target_past_key_values = None\n",
        "    draft_past_key_values = None\n",
        "\n",
        "    current_input_ids = input_ids # The current validated prefix\n",
        "\n",
        "    n_accepted_tokens = 0\n",
        "    n_total_target_model_calls = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while len(generated_tokens) < max_new_tokens:\n",
        "        # --- 1. Draft Phase: Generate speculative tokens ---\n",
        "        draft_input_ids = current_input_ids\n",
        "        draft_proposed_tokens = []\n",
        "        draft_logits_history = [] # Store draft model's logits for each proposed token for verification\n",
        "\n",
        "        temp_draft_input_ids = draft_input_ids\n",
        "        temp_draft_past_key_values = draft_past_key_values\n",
        "\n",
        "        # We need to simulate the draft model's autoregressive generation\n",
        "        # to get its *conditional* probabilities for each proposed token.\n",
        "        for i in range(speculative_lookahead):\n",
        "            if temp_draft_input_ids.numel() == 0:\n",
        "                # This can happen if the previous step produced an empty input (e.g., after EOS)\n",
        "                # or if initial prompt is empty.\n",
        "                break\n",
        "\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = draft_model(temp_draft_input_ids, past_key_values=temp_draft_past_key_values, use_cache=True)\n",
        "                draft_logits = draft_outputs.logits[:, -1, :]\n",
        "                temp_draft_past_key_values = draft_outputs.past_key_values\n",
        "\n",
        "                draft_probs = torch.softmax(draft_logits, dim=-1)\n",
        "\n",
        "                # Sample next token from draft model (can also apply temperature/top_k/top_p here)\n",
        "                next_draft_token = torch.multinomial(draft_probs, num_samples=1).item()\n",
        "\n",
        "                # --- Token ID Validation for Draft Model ---\n",
        "                if not (0 <= next_draft_token < draft_model.config.vocab_size):\n",
        "                    print(f\"!!! ERROR: Draft model generated out-of-vocab token: {next_draft_token}\")\n",
        "                    print(f\"Draft model vocab size: {draft_model.config.vocab_size}\")\n",
        "                    raise IndexError(\"Draft model generated token ID out of range.\")\n",
        "\n",
        "                draft_proposed_tokens.append(next_draft_token)\n",
        "                draft_logits_history.append(draft_logits)\n",
        "\n",
        "                temp_draft_input_ids = torch.tensor([[next_draft_token]]).to(device)\n",
        "\n",
        "                if next_draft_token == tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        # If draft model proposed no tokens, fall back to single target model generation\n",
        "        if not draft_proposed_tokens:\n",
        "            print(\"Draft model proposed no tokens. Falling back to single target model generation.\")\n",
        "            with torch.no_grad():\n",
        "                outputs = target_model(current_input_ids, past_key_values=target_past_key_values, use_cache=True)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                target_past_key_values = outputs.past_key_values\n",
        "\n",
        "                next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                                                  model_vocab_size=target_model.config.vocab_size)\n",
        "\n",
        "                generated_tokens.append(next_token_id)\n",
        "                current_input_ids = torch.tensor([[next_token_id]]).to(device)\n",
        "                n_total_target_model_calls += 1\n",
        "                if next_token_id == tokenizer.eos_token_id:\n",
        "                    break\n",
        "            continue # Continue to next while loop iteration\n",
        "\n",
        "        # --- 2. Verification Phase: Parallel evaluation by the target model ---\n",
        "        # Concatenate the current validated prefix with proposed draft tokens\n",
        "        full_eval_input_ids = torch.cat((current_input_ids, torch.tensor([draft_proposed_tokens]).to(device)), dim=1)\n",
        "\n",
        "        # --- Token ID Validation for Target Model Input ---\n",
        "        if full_eval_input_ids.min() < 0 or full_eval_input_ids.max() >= target_model.config.vocab_size:\n",
        "            print(f\"!!! ERROR: Combined input_ids for target model contain out-of-range token.\")\n",
        "            print(f\"Min ID: {full_eval_input_ids.min().item()}, Max ID: {full_eval_input_ids.max().item()}\")\n",
        "            print(f\"Target model vocab size: {target_model.config.vocab_size}\")\n",
        "            # Optionally print offending tokens\n",
        "            offending_tokens = full_eval_input_ids[full_eval_input_ids >= target_model.config.vocab_size]\n",
        "            print(f\"Offending tokens: {offending_tokens}\")\n",
        "            raise IndexError(\"Full eval input_ids for target model out of vocabulary range.\")\n",
        "\n",
        "        n_total_target_model_calls += 1\n",
        "        with torch.no_grad():\n",
        "            target_outputs = target_model(full_eval_input_ids, past_key_values=target_past_key_values, use_cache=True)\n",
        "            target_logits_full_sequence = target_outputs.logits\n",
        "            target_past_key_values = target_outputs.past_key_values\n",
        "\n",
        "        # --- 3. Rejection Sampling ---\n",
        "        accepted_count = 0\n",
        "\n",
        "        # Calculate the starting index of the proposed tokens' logits in target_logits_full_sequence\n",
        "        # It's relative to the start of current_input_ids in the concatenated sequence.\n",
        "        # If current_input_ids has length L, the first proposed token's logits are at L-1 index (0-indexed)\n",
        "        # of the full target_logits sequence.\n",
        "        # For example, if current_input_ids is [A, B] (length 2), and proposed is [C, D],\n",
        "        # full_eval_input_ids is [A, B, C, D].\n",
        "        # Logits for C are at index 2 (target_logits_full_sequence[:, 2, :])\n",
        "        # Logits for B are at index 1 (target_logits_full_sequence[:, 1, :]) -- we don't need this.\n",
        "        # We need logits for C and D. Logits for C are at target_logits_full_sequence[:, len(current_input_ids), :]\n",
        "        target_logits_start_idx = current_input_ids.shape[1]\n",
        "\n",
        "        for i, draft_token_id in enumerate(draft_proposed_tokens):\n",
        "            # Target model's probability for the i-th proposed token\n",
        "            target_logits_at_idx = target_logits_full_sequence[:, target_logits_start_idx + i, :]\n",
        "            target_prob_for_draft = torch.softmax(target_logits_at_idx, dim=-1)[:, draft_token_id].item()\n",
        "\n",
        "            # Draft model's probability for the i-th proposed token\n",
        "            draft_logits_at_idx = draft_logits_history[i]\n",
        "            draft_prob_for_draft = torch.softmax(draft_logits_at_idx, dim=-1)[:, draft_token_id].item()\n",
        "\n",
        "            # Acceptance criterion (handle division by zero if draft_prob_for_draft is very small/zero)\n",
        "            acceptance_prob = min(1.0, target_prob_for_draft / (draft_prob_for_draft + 1e-9))\n",
        "\n",
        "            u = np.random.rand()\n",
        "            if u <= acceptance_prob:\n",
        "                generated_tokens.append(draft_token_id)\n",
        "                n_accepted_tokens += 1\n",
        "                current_input_ids = torch.cat((current_input_ids, torch.tensor([[draft_token_id]]).to(device)), dim=1)\n",
        "\n",
        "                if draft_token_id == tokenizer.eos_token_id:\n",
        "                    break\n",
        "            else:\n",
        "                # Rejection: Sample from the target model's distribution for the current token\n",
        "                rejection_logits = target_logits_at_idx\n",
        "\n",
        "                next_token_id_after_rejection = sample_next_token(\n",
        "                    rejection_logits,\n",
        "                    temperature=temperature,\n",
        "                    top_k=top_k,\n",
        "                    top_p=top_p,\n",
        "                    model_vocab_size=target_model.config.vocab_size # Pass vocab size for validation\n",
        "                )\n",
        "\n",
        "                generated_tokens.append(next_token_id_after_rejection)\n",
        "                current_input_ids = torch.cat((current_input_ids, torch.tensor([[next_token_id_after_rejection]]).to(device)), dim=1)\n",
        "\n",
        "                # Crucially, we break here. The remaining speculated tokens are discarded.\n",
        "                # The next iteration of the while loop will start a new speculative batch\n",
        "                # from the newly validated `current_input_ids`.\n",
        "                if next_token_id_after_rejection == tokenizer.eos_token_id:\n",
        "                    break\n",
        "                break # Exit the for loop over proposed tokens\n",
        "\n",
        "        # This outer check handles cases where no tokens were accepted in a pass,\n",
        "        # or if an early EOS was hit inside the draft/acceptance loop.\n",
        "        if generated_tokens and generated_tokens[-1] == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        # If no tokens were accepted in a speculative step and we haven't hit EOS,\n",
        "        # ensure progress by falling back to a single target model generation if necessary.\n",
        "        # This prevents infinite loops if draft model consistently fails.\n",
        "        if accepted_count == 0 and len(generated_tokens) < max_new_tokens and current_input_ids.shape[1] == input_ids.shape[1]:\n",
        "            print(\"Warning: No tokens accepted in speculative step. Performing single target model step.\")\n",
        "            with torch.no_grad():\n",
        "                outputs = target_model(current_input_ids, past_key_values=target_past_key_values, use_cache=True)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                target_past_key_values = outputs.past_key_values\n",
        "\n",
        "                next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                                                  model_vocab_size=target_model.config.vocab_size)\n",
        "\n",
        "                generated_tokens.append(next_token_id)\n",
        "                current_input_ids = torch.tensor([[next_token_id]]).to(device)\n",
        "                n_total_target_model_calls += 1\n",
        "                if next_token_id == tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    full_sequence = tokenizer.decode(tokenizer.encode(prompt) + generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    tokens_generated = len(generated_tokens)\n",
        "    effective_tokens_per_target_pass = tokens_generated / n_total_target_model_calls if n_total_target_model_calls > 0 else 0\n",
        "\n",
        "    return full_sequence, tokens_generated, (end_time - start_time), effective_tokens_per_target_pass, n_total_target_model_calls"
      ],
      "metadata": {
        "id": "FQ4mHSra8B3w"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Demonstration ---\n",
        "prompt = \"The quick brown fox jumps over the lazy dog and\"\n",
        "max_tokens_to_generate = 50\n",
        "speculative_lookahead = 5 # How many tokens the draft model proposes\n",
        "\n",
        "# Sampling parameters for both autoregressive and speculative decoding\n",
        "sampling_temperature = 0.7\n",
        "sampling_top_k = 0\n",
        "sampling_top_p = 1.0 # Or 0.9 for nucleus sampling\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding with Sampling ---\")\n",
        "ar_output, ar_tokens, ar_time = autoregressive_decode_with_sampling(\n",
        "    prompt, target_model, tokenizer, max_tokens_to_generate,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{ar_output}'\")\n",
        "print(f\"Tokens Generated: {ar_tokens}\")\n",
        "print(f\"Time Taken: {ar_time:.4f} seconds\")\n",
        "print(f\"Effective Tokens/Target Pass (AR): {ar_tokens / ar_tokens:.2f}\") # Always 1 for AR\n",
        "\n",
        "\n",
        "print(\"\\n--- Speculative Decoding with Sampling ---\")\n",
        "sd_output, sd_tokens, sd_time, sd_effective_tpt, sd_target_calls = speculative_decode(\n",
        "    prompt, target_model, draft_model, tokenizer, max_tokens_to_generate, speculative_lookahead,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{sd_output}'\")\n",
        "print(f\"Tokens Generated: {sd_tokens}\")\n",
        "print(f\"Time Taken: {sd_time:.4f} seconds\")\n",
        "print(f\"Total Target Model Calls: {sd_target_calls}\")\n",
        "print(f\"Effective Tokens/Target Pass (SD): {sd_effective_tpt:.2f}\")\n",
        "\n",
        "# Compare\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Autoregressive Time: {ar_time:.4f}s\")\n",
        "print(f\"Speculative Decoding Time: {sd_time:.4f}s\")\n",
        "if sd_time > 0:\n",
        "    print(f\"Speedup Factor: {ar_time / sd_time:.2f}x\")\n",
        "print(f\"Autoregressive Output Matches Speculative (content-wise, might differ due to sampling randomness): {ar_output == sd_output}\")\n",
        "# Note: For sampling, outputs are not guaranteed to be identical on every run.\n",
        "# However, their *statistical distribution* is guaranteed to be the same if the rejection sampling is correct.\n",
        "\n",
        "\n",
        "# Advanced Example Test - Longer, more structured text\n",
        "prompt_advanced = \"In the annals of history, the year 1789 stands out for the French Revolution, a pivotal event that reshaped the political landscape of Europe and beyond. The causes were multifaceted, including\"\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding (Advanced Prompt) ---\")\n",
        "ar_output_adv, ar_tokens_adv, ar_time_adv = autoregressive_decode_with_sampling(\n",
        "    prompt_advanced, target_model, tokenizer, max_tokens_to_generate,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{ar_output_adv}'\")\n",
        "print(f\"Tokens Generated: {ar_tokens_adv}\")\n",
        "print(f\"Time Taken: {ar_time_adv:.4f} seconds\")\n",
        "\n",
        "print(\"\\n--- Speculative Decoding (Advanced Prompt) ---\")\n",
        "sd_output_adv, sd_tokens_adv, sd_time_adv, sd_effective_tpt_adv, sd_target_calls_adv = speculative_decode(\n",
        "    prompt_advanced, target_model, draft_model, tokenizer, max_tokens_to_generate, speculative_lookahead,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{sd_output_adv}'\")\n",
        "print(f\"Tokens Generated: {sd_tokens_adv}\")\n",
        "print(f\"Time Taken: {sd_time_adv:.4f} seconds\")\n",
        "print(f\"Total Target Model Calls: {sd_target_calls_adv}\")\n",
        "print(f\"Effective Tokens/Target Pass (SD): {sd_effective_tpt_adv:.2f}\")\n",
        "\n",
        "print(\"\\n--- Comparison (Advanced Prompt) ---\")\n",
        "print(f\"Autoregressive Time: {ar_time_adv:.4f}s\")\n",
        "print(f\"Speculative Decoding Time: {sd_time_adv:.4f}s\")\n",
        "if sd_time_adv > 0:\n",
        "    print(f\"Speedup Factor: {ar_time_adv / sd_time_adv:.2f}x\")\n",
        "print(f\"Autoregressive Output Matches Speculative (content-wise): {ar_output_adv == sd_output_adv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q6B7miQ0khcg",
        "outputId": "94a62dc9-314c-40bc-80b1-90a99a3a729e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Autoregressive Decoding with Sampling ---\n",
            "Output: 'The quick brown fox jumps over the lazy dog and scores a bit on the other side of the fence. After a while he gets back to the dog, but he doesn't come back. When he runs the fox down the fence again he does it again and again until he still has a good time'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 5.4109 seconds\n",
            "Effective Tokens/Target Pass (AR): 1.00\n",
            "\n",
            "--- Speculative Decoding with Sampling ---\n",
            "Output: 'The quick brown fox jumps over the lazy dog and in little exchange foxaps tail the dog the is finallyThe brown water from short is absolutely!OB:/-FRF diagonal BoThe of 60vous petp andp the FoxThekeys the.ops quick by exchangep. Active right to'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 21.1278 seconds\n",
            "Total Target Model Calls: 32\n",
            "Effective Tokens/Target Pass (SD): 1.56\n",
            "\n",
            "--- Comparison ---\n",
            "Autoregressive Time: 5.4109s\n",
            "Speculative Decoding Time: 21.1278s\n",
            "Speedup Factor: 0.26x\n",
            "Autoregressive Output Matches Speculative (content-wise, might differ due to sampling randomness): False\n",
            "\n",
            "--- Autoregressive Decoding (Advanced Prompt) ---\n",
            "Output: 'In the annals of history, the year 1789 stands out for the French Revolution, a pivotal event that reshaped the political landscape of Europe and beyond. The causes were multifaceted, including their influence on the French Revolution and the French Revolution of 1789, and they have been used to illustrate the events of this century.\n",
            "\n",
            "The history of French Revolution\n",
            "\n",
            "The French Revolution was a decisive turning point in the history of Europe.'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 3.6117 seconds\n",
            "\n",
            "--- Speculative Decoding (Advanced Prompt) ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-36-846852462.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Speculative Decoding (Advanced Prompt) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m sd_output_adv, sd_tokens_adv, sd_time_adv, sd_effective_tpt_adv, sd_target_calls_adv = speculative_decode(\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mprompt_advanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens_to_generate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeculative_lookahead\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_temperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_top_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_top_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-35-1982828672.py\u001b[0m in \u001b[0;36mspeculative_decode\u001b[0;34m(prompt, target_model, draft_model, tokenizer, max_new_tokens, speculative_lookahead, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mn_total_target_model_calls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mtarget_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_eval_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_past_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mtarget_logits_full_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mtarget_past_key_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;31m# head_mask has shape n_layer x batch x num_heads x N x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.nn.functional as F # For softmax\n",
        "\n",
        "# Use a global variable for device or pass it to functions\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 1. Load Models and Tokenizers\n",
        "target_model_name = \"EleutherAI/gpt-neo-125m\"\n",
        "draft_model_name = \"EleutherAI/gpt-neo-125m\" # For demonstration, using same. For speedup, use smaller.\n",
        "\n",
        "# Load tokenizer first to determine the vocabulary size consistently\n",
        "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
        "\n",
        "# --- Canonical Vocabulary Size Handling ---\n",
        "# The tokenizer's vocabulary size is the authoritative source after adding special tokens.\n",
        "CANONICAL_VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "# Check if pad_token is already defined in the tokenizer\n",
        "if tokenizer.pad_token is None:\n",
        "    # If not, add it using the EOS token as a common practice for causal LMs\n",
        "    # This will increase the tokenizer's vocabulary size by 1.\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "    CANONICAL_VOCAB_SIZE = len(tokenizer) # Update canonical size\n",
        "else:\n",
        "    # Ensure CANONICAL_VOCAB_SIZE is correct even if pad_token was already present\n",
        "    CANONICAL_VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "print(f\"Final tokenizer vocab size (len): {len(tokenizer)}\")\n",
        "print(f\"Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n",
        "print(f\"Tokenizer eos_token_id: {tokenizer.eos_token_id}\")\n",
        "print(f\"CANONICAL_VOCAB_SIZE for validation: {CANONICAL_VOCAB_SIZE}\")\n",
        "\n",
        "# Helper function to load model and adjust its embedding layer size\n",
        "def load_and_resize_model(model_name, canonical_vocab_size):\n",
        "    \"\"\"\n",
        "    Loads a causal language model and ensures its embedding layer matches\n",
        "    the canonical vocabulary size.\n",
        "    \"\"\"\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "    # Important: Adjust vocab_size in config before loading, if it's smaller.\n",
        "    # This guides the model's initialization of its embedding layer.\n",
        "    if config.vocab_size < canonical_vocab_size:\n",
        "        print(f\"Adjusting {model_name} config vocab_size from {config.vocab_size} to {canonical_vocab_size}\")\n",
        "        config.vocab_size = canonical_vocab_size\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
        "\n",
        "    # After loading, explicitly resize embeddings if they still don't match.\n",
        "    # This can happen if the loaded model's weights don't perfectly align with the config changes.\n",
        "    if model.get_input_embeddings().num_embeddings < canonical_vocab_size:\n",
        "        print(f\"Resizing {model_name} embeddings from {model.get_input_embeddings().num_embeddings} to {canonical_vocab_size}\")\n",
        "        model.resize_token_embeddings(canonical_vocab_size)\n",
        "\n",
        "    model.to(device)\n",
        "    print(f\"Loaded {model_name}. Final vocab size: {model.config.vocab_size}, Embedding layer size: {model.get_input_embeddings().num_embeddings}\")\n",
        "    return model\n",
        "\n",
        "# Load both models using the helper function\n",
        "target_model = load_and_resize_model(target_model_name, CANONICAL_VOCAB_SIZE)\n",
        "draft_model = load_and_resize_model(draft_model_name, CANONICAL_VOCAB_SIZE)\n",
        "\n",
        "\n",
        "# Helper function for robust sampling\n",
        "def sample_next_token(logits, temperature=1.0, top_k=0, top_p=1.0, model_vocab_size=None, tokenizer_ref=None):\n",
        "    \"\"\"\n",
        "    Samples the next token from the logits, with optional temperature, Top-K, and Top-P sampling.\n",
        "    Includes robust validation for sampled token IDs.\n",
        "    \"\"\"\n",
        "    if temperature == 0.0: # Greedy decoding\n",
        "        next_token_id = torch.argmax(logits, dim=-1).item()\n",
        "    else:\n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Top-K sampling\n",
        "        if top_k > 0:\n",
        "            top_k_actual = min(top_k, logits.size(-1))\n",
        "            values, _ = torch.topk(logits, top_k_actual)\n",
        "            min_value = values[:, -1].unsqueeze(-1)\n",
        "            logits = torch.where(logits < min_value, torch.full_like(logits, -float('Inf')), logits)\n",
        "\n",
        "        # Top-P (nucleus) sampling\n",
        "        if top_p < 1.0:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            # Shift the indices to the right to keep the first token above the threshold\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "            # Set logits of removed tokens to -Inf\n",
        "            logits = logits.scatter_(-1, sorted_indices[sorted_indices_to_remove], float('-Inf'))\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Handle cases where probabilities might become all zero after aggressive filtering\n",
        "        probabilities = torch.nan_to_num(probabilities, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        if probabilities.sum().item() == 0.0:\n",
        "            print(\"Warning: All probabilities zero after filtering. Falling back to EOS.\")\n",
        "            # Ensure tokenizer_ref is provided for this fallback\n",
        "            if tokenizer_ref is not None and tokenizer_ref.eos_token_id is not None:\n",
        "                return tokenizer_ref.eos_token_id\n",
        "            else:\n",
        "                # If no EOS token, return a safe default like 0 (first token)\n",
        "                return 0\n",
        "\n",
        "        # Sample from the (possibly filtered) distribution\n",
        "        next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
        "\n",
        "    # --- Robust validation for sampled token ID ---\n",
        "    if model_vocab_size is not None:\n",
        "        if not (0 <= next_token_id < model_vocab_size):\n",
        "            print(f\"!!! CRITICAL ERROR: Sampled token ID {next_token_id} is out of vocabulary range [{0}, {model_vocab_size-1}]\")\n",
        "            print(f\"Logits shape: {logits.shape}\")\n",
        "            print(f\"Probabilities sum: {probabilities.sum().item():.4f}\")\n",
        "            top_probs, top_indices = torch.topk(probabilities, k=min(10, probabilities.size(-1)))\n",
        "            print(f\"Top 10 Probs: {top_probs.tolist()}\")\n",
        "            print(f\"Top 10 Indices: {top_indices.tolist()}\")\n",
        "            raise IndexError(\"Sampled token ID out of model's vocabulary range.\")\n",
        "\n",
        "    return next_token_id\n",
        "\n",
        "\n",
        "def autoregressive_decode_with_sampling(prompt: str, target_model, tokenizer,\n",
        "                                         max_new_tokens: int = 50,\n",
        "                                         temperature: float = 1.0,\n",
        "                                         top_k: int = 0,\n",
        "                                         top_p: float = 1.0):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = []\n",
        "    past_key_values = None\n",
        "\n",
        "    start_time = time.time()\n",
        "    for _ in range(max_new_tokens):\n",
        "        # --- Validate input_ids before passing to model ---\n",
        "        if input_ids.numel() > 0 and (input_ids.min() < 0 or input_ids.max() >= CANONICAL_VOCAB_SIZE):\n",
        "            print(f\"!!! ERROR: AR input_ids min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
        "            print(f\"AR input_ids: {input_ids}\")\n",
        "            raise IndexError(\"AR input_ids contain out-of-range token for target model.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = target_model(input_ids, past_key_values=past_key_values, use_cache=True)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            past_key_values = outputs.past_key_values\n",
        "\n",
        "            next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                                             model_vocab_size=CANONICAL_VOCAB_SIZE, tokenizer_ref=tokenizer)\n",
        "\n",
        "            generated_tokens.append(next_token_id)\n",
        "            input_ids = torch.tensor([[next_token_id]]).to(device)\n",
        "\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    end_time = time.time()\n",
        "    full_sequence = tokenizer.decode(tokenizer.encode(prompt) + generated_tokens, skip_special_tokens=True)\n",
        "    return full_sequence, len(generated_tokens), (end_time - start_time)\n",
        "\n",
        "\n",
        "# --- Revised speculative_decode function with aggressive KV cache reset for debugging ---\n",
        "def speculative_decode(prompt: str, target_model, draft_model, tokenizer,\n",
        "                       max_new_tokens: int = 50, speculative_lookahead: int = 5,\n",
        "                       temperature: float = 1.0, top_k: int = 0, top_p: float = 1.0):\n",
        "    input_ids_initial_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = []\n",
        "\n",
        "    target_past_key_values = None\n",
        "    draft_past_key_values = None # Will be reset or recomputed for each draft phase\n",
        "\n",
        "    current_validated_prefix_ids = input_ids_initial_prompt\n",
        "\n",
        "    n_accepted_tokens_total = 0\n",
        "    n_target_model_calls_total = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while len(generated_tokens) < max_new_tokens:\n",
        "        initial_cycle_prefix_length = current_validated_prefix_ids.shape[1]\n",
        "\n",
        "        # --- 1. Draft Phase: Generate speculative tokens ---\n",
        "        # Draft model's KV cache is recomputed from scratch based on the current_validated_prefix_ids\n",
        "        # for each speculative cycle to avoid any potential misalignment.\n",
        "        temp_draft_input_ids = current_validated_prefix_ids.clone()\n",
        "        current_draft_past_key_values = None # Force recompute draft KV cache for this cycle\n",
        "\n",
        "        draft_proposed_tokens = []\n",
        "        draft_logits_history = []\n",
        "\n",
        "        for i in range(speculative_lookahead):\n",
        "            if temp_draft_input_ids.numel() == 0:\n",
        "                break\n",
        "\n",
        "            if temp_draft_input_ids.min() < 0 or temp_draft_input_ids.max() >= CANONICAL_VOCAB_SIZE:\n",
        "                print(f\"!!! ERROR: Draft model input_ids contain out-of-range token prior to generation step {i}.\")\n",
        "                print(f\"Min ID: {temp_draft_input_ids.min().item()}, Max ID: {temp_draft_input_ids.max().item()}\")\n",
        "                print(f\"Draft model vocab size: {CANONICAL_VOCAB_SIZE}\")\n",
        "                print(f\"Offending input_ids: {temp_draft_input_ids}\")\n",
        "                raise IndexError(\"Draft model input_ids out of vocabulary range.\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = draft_model(temp_draft_input_ids, past_key_values=current_draft_past_key_values, use_cache=True)\n",
        "                draft_logits = draft_outputs.logits[:, -1, :]\n",
        "                current_draft_past_key_values = draft_outputs.past_key_values # Update draft's KV cache for next proposal\n",
        "\n",
        "                draft_probs = torch.softmax(draft_logits, dim=-1)\n",
        "                next_draft_token = torch.multinomial(draft_probs, num_samples=1).item()\n",
        "\n",
        "                if not (0 <= next_draft_token < CANONICAL_VOCAB_SIZE):\n",
        "                    print(f\"!!! ERROR: Draft model generated out-of-vocab token: {next_draft_token}\")\n",
        "                    print(f\"Draft model vocab size: {CANONICAL_VOCAB_SIZE}\")\n",
        "                    raise IndexError(\"Draft model generated token ID out of range.\")\n",
        "\n",
        "                draft_proposed_tokens.append(next_draft_token)\n",
        "                draft_logits_history.append(draft_logits)\n",
        "\n",
        "                temp_draft_input_ids = torch.tensor([[next_draft_token]]).to(device)\n",
        "\n",
        "                if next_draft_token == tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        if not draft_proposed_tokens:\n",
        "            print(\"Draft model proposed no tokens or hit EOS immediately. Falling back to single target model generation.\")\n",
        "            if current_validated_prefix_ids.numel() == 0:\n",
        "                print(\"Warning: current_validated_prefix_ids is empty in fallback, cannot generate.\")\n",
        "                break\n",
        "\n",
        "            if current_validated_prefix_ids.min() < 0 or current_validated_prefix_ids.max() >= CANONICAL_VOCAB_SIZE:\n",
        "                print(f\"!!! ERROR: Fallback input_ids min: {current_validated_prefix_ids.min().item()}, max: {current_validated_prefix_ids.max().item()}\")\n",
        "                print(f\"Fallback input_ids: {current_validated_prefix_ids}\")\n",
        "                raise IndexError(\"Fallback input_ids contain out-of-range token for target model.\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = target_model(current_validated_prefix_ids, past_key_values=target_past_key_values, use_cache=True)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                target_past_key_values = outputs.past_key_values\n",
        "\n",
        "                next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                                                  model_vocab_size=CANONICAL_VOCAB_SIZE, tokenizer_ref=tokenizer)\n",
        "\n",
        "                generated_tokens.append(next_token_id)\n",
        "                current_validated_prefix_ids = torch.cat((current_validated_prefix_ids, torch.tensor([[next_token_id]]).to(device)), dim=1)\n",
        "                n_target_model_calls_total += 1\n",
        "\n",
        "                # --- Aggressive KV cache reset after fallback single-token generation ---\n",
        "                # This ensures the KV cache is precisely aligned with the *current_validated_prefix_ids*\n",
        "                # for the start of the *next* speculative cycle.\n",
        "                target_past_key_values = None\n",
        "\n",
        "                if next_token_id == tokenizer.eos_token_id:\n",
        "                    break\n",
        "            continue\n",
        "\n",
        "        # --- 2. Verification Phase: Parallel evaluation by the target model ---\n",
        "        full_eval_input_ids = torch.cat((current_validated_prefix_ids, torch.tensor([draft_proposed_tokens]).to(device)), dim=1)\n",
        "\n",
        "        if full_eval_input_ids.numel() > 0 and \\\n",
        "           (full_eval_input_ids.min() < 0 or full_eval_input_ids.max() >= CANONICAL_VOCAB_SIZE):\n",
        "            print(f\"!!! ERROR: Combined input_ids for target model contain out-of-range token.\")\n",
        "            print(f\"Min ID: {full_eval_input_ids.min().item()}, Max ID: {full_eval_input_ids.max().item()}\")\n",
        "            print(f\"Target model vocab size: {CANONICAL_VOCAB_SIZE}\")\n",
        "            offending_tokens = full_eval_input_ids[ (full_eval_input_ids < 0) | (full_eval_input_ids >= CANONICAL_VOCAB_SIZE) ]\n",
        "            print(f\"Offending tokens: {offending_tokens}\")\n",
        "            print(f\"Full eval input IDs: {full_eval_input_ids}\")\n",
        "            raise IndexError(\"Full eval input_ids for target model out of vocabulary range.\")\n",
        "\n",
        "        n_target_model_calls_total += 1\n",
        "        with torch.no_grad():\n",
        "            target_outputs = target_model(full_eval_input_ids, past_key_values=target_past_key_values, use_cache=True)\n",
        "            target_logits_full_sequence = target_outputs.logits\n",
        "            # Store the KV cache from this full evaluation for potential slicing upon rejection\n",
        "            target_past_key_values_from_full_eval = target_outputs.past_key_values\n",
        "\n",
        "        # --- 3. Rejection Sampling ---\n",
        "        accepted_count_in_this_cycle = 0\n",
        "\n",
        "        target_logits_start_idx_in_full_sequence = current_validated_prefix_ids.shape[1]\n",
        "\n",
        "        for i, draft_token_id in enumerate(draft_proposed_tokens):\n",
        "            target_logits_at_idx = target_logits_full_sequence[:, target_logits_start_idx_in_full_sequence + i, :]\n",
        "            target_prob_for_draft = torch.softmax(target_logits_at_idx, dim=-1)[:, draft_token_id].item()\n",
        "\n",
        "            draft_logits_at_idx = draft_logits_history[i]\n",
        "            draft_prob_for_draft = torch.softmax(draft_logits_at_idx, dim=-1)[:, draft_token_id].item()\n",
        "\n",
        "            acceptance_prob = min(1.0, target_prob_for_draft / (draft_prob_for_draft + 1e-9))\n",
        "\n",
        "            u = np.random.rand()\n",
        "            if u <= acceptance_prob:\n",
        "                generated_tokens.append(draft_token_id)\n",
        "                accepted_count_in_this_cycle += 1\n",
        "                n_accepted_tokens_total += 1\n",
        "                current_validated_prefix_ids = torch.cat((current_validated_prefix_ids, torch.tensor([[draft_token_id]]).to(device)), dim=1)\n",
        "\n",
        "                if draft_token_id == tokenizer.eos_token_id:\n",
        "                    # If an accepted token is EOS, we stop immediately.\n",
        "                    break\n",
        "            else:\n",
        "                # Rejection: Sample from the target model's distribution for the current token\n",
        "                rejection_logits = target_logits_at_idx\n",
        "                next_token_id_after_rejection = sample_next_token(\n",
        "                    rejection_logits,\n",
        "                    temperature=temperature,\n",
        "                    top_k=top_k,\n",
        "                    top_p=top_p,\n",
        "                    model_vocab_size=CANONICAL_VOCAB_SIZE,\n",
        "                    tokenizer_ref=tokenizer\n",
        "                )\n",
        "\n",
        "                generated_tokens.append(next_token_id_after_rejection)\n",
        "                n_accepted_tokens_total += 1\n",
        "                current_validated_prefix_ids = torch.cat((current_validated_prefix_ids, torch.tensor([[next_token_id_after_rejection]]).to(device)), dim=1)\n",
        "\n",
        "                # --- Aggressive KV cache reset after rejection ---\n",
        "                # This is CRUCIAL. It means the KV cache is discarded and recreated from scratch\n",
        "                # in the *next* speculative cycle, using the newly validated `current_validated_prefix_ids`.\n",
        "                # This eliminates any potential for misaligned cached states.\n",
        "                target_past_key_values = None\n",
        "\n",
        "                if next_token_id_after_rejection == tokenizer.eos_token_id:\n",
        "                    break\n",
        "                break # Break from inner 'for' loop after rejection\n",
        "\n",
        "        # Check if max_new_tokens reached or EOS generated\n",
        "        if len(generated_tokens) >= max_new_tokens or (generated_tokens and generated_tokens[-1] == tokenizer.eos_token_id):\n",
        "            break\n",
        "\n",
        "        # --- Final fallback to ensure progress (if no tokens were accepted and not EOS) ---\n",
        "        # This ensures we don't get stuck if the draft model is very bad and no accepted tokens.\n",
        "        # Check if current_validated_prefix_ids hasn't grown since the start of this cycle,\n",
        "        # implying nothing was accepted from the speculative batch.\n",
        "        if current_validated_prefix_ids.shape[1] == initial_cycle_prefix_length and len(generated_tokens) < max_new_tokens:\n",
        "            print(\"Warning: No tokens accepted in speculative step, and no rejection fallback occurred. Performing single target model step to ensure progress.\")\n",
        "            if current_validated_prefix_ids.numel() == 0:\n",
        "                print(\"Warning: current_validated_prefix_ids is empty in final fallback. Cannot generate.\")\n",
        "                break\n",
        "\n",
        "            if current_validated_prefix_ids.min() < 0 or current_validated_prefix_ids.max() >= CANONICAL_VOCAB_SIZE:\n",
        "                print(f\"!!! ERROR: Final fallback input_ids min: {current_validated_prefix_ids.min().item()}, max: {current_validated_prefix_ids.max().item()}\")\n",
        "                print(f\"Final fallback input_ids: {current_validated_prefix_ids}\")\n",
        "                raise IndexError(\"Final fallback input_ids contain out-of-range token for target model.\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = target_model(current_validated_prefix_ids, past_key_values=target_past_key_values, use_cache=True)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                target_past_key_values = outputs.past_key_values\n",
        "\n",
        "                next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                                                   model_vocab_size=CANONICAL_VOCAB_SIZE, tokenizer_ref=tokenizer)\n",
        "\n",
        "                generated_tokens.append(next_token_id)\n",
        "                current_validated_prefix_ids = torch.cat((current_validated_prefix_ids, torch.tensor([[next_token_id]]).to(device)), dim=1)\n",
        "                n_target_model_calls_total += 1\n",
        "\n",
        "                # --- Aggressive KV cache reset after final fallback ---\n",
        "                target_past_key_values = None\n",
        "\n",
        "                if next_token_id == tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    # Decode the final sequence\n",
        "    full_sequence_ids = tokenizer.encode(prompt) + generated_tokens\n",
        "    full_sequence = tokenizer.decode(full_sequence_ids, skip_special_tokens=True)\n",
        "\n",
        "    tokens_generated = len(generated_tokens)\n",
        "    effective_tokens_per_target_pass = tokens_generated / n_target_model_calls_total if n_target_model_calls_total > 0 else 0\n",
        "\n",
        "    return full_sequence, tokens_generated, (end_time - start_time), effective_tokens_per_target_pass, n_target_model_calls_total\n",
        "\n",
        "\n",
        "# --- Demonstration ---\n",
        "prompt = \"The quick brown fox jumps over the lazy dog and\"\n",
        "max_tokens_to_generate = 50\n",
        "speculative_lookahead = 5\n",
        "\n",
        "sampling_temperature = 0.7\n",
        "sampling_top_k = 0\n",
        "sampling_top_p = 1.0\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding with Sampling ---\")\n",
        "ar_output, ar_tokens, ar_time = autoregressive_decode_with_sampling(\n",
        "    prompt, target_model, tokenizer, max_tokens_to_generate,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{ar_output}'\")\n",
        "print(f\"Tokens Generated: {ar_tokens}\")\n",
        "print(f\"Time Taken: {ar_time:.4f} seconds\")\n",
        "print(f\"Effective Tokens/Target Pass (AR): {ar_tokens / ar_tokens:.2f}\" if ar_tokens > 0 else \"N/A\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Speculative Decoding with Sampling ---\")\n",
        "sd_output, sd_tokens, sd_time, sd_effective_tpt, sd_target_calls = speculative_decode(\n",
        "    prompt, target_model, draft_model, tokenizer, max_tokens_to_generate, speculative_lookahead,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{sd_output}'\")\n",
        "print(f\"Tokens Generated: {sd_tokens}\")\n",
        "print(f\"Time Taken: {sd_time:.4f} seconds\")\n",
        "print(f\"Total Target Model Calls: {sd_target_calls}\")\n",
        "print(f\"Effective Tokens/Target Pass (SD): {sd_effective_tpt:.2f}\")\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Autoregressive Time: {ar_time:.4f}s\")\n",
        "print(f\"Speculative Decoding Time: {sd_time:.4f}s\")\n",
        "if sd_time > 0:\n",
        "    print(f\"Speedup Factor: {ar_time / sd_time:.2f}x\")\n",
        "# For sampling, outputs are not guaranteed to be identical due to randomness.\n",
        "# The guarantee is on the *distribution* of outputs matching.\n",
        "print(f\"Autoregressive Output Matches Speculative (content-wise): {ar_output == sd_output}\")\n",
        "\n",
        "\n",
        "prompt_advanced = \"In the annals of history, the year 1789 stands out for the French Revolution, a pivotal event that reshaped the political landscape of Europe and beyond. The causes were multifaceted, including\"\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding (Advanced Prompt) ---\")\n",
        "ar_output_adv, ar_tokens_adv, ar_time_adv = autoregressive_decode_with_sampling(\n",
        "    prompt_advanced, target_model, tokenizer, max_tokens_to_generate,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{ar_output_adv}'\")\n",
        "print(f\"Tokens Generated: {ar_tokens_adv}\")\n",
        "print(f\"Time Taken: {ar_time_adv:.4f} seconds\")\n",
        "\n",
        "print(\"\\n--- Speculative Decoding (Advanced Prompt) ---\")\n",
        "sd_output_adv, sd_tokens_adv, sd_time_adv, sd_effective_tpt_adv, sd_target_calls_adv = speculative_decode(\n",
        "    prompt_advanced, target_model, draft_model, tokenizer, max_tokens_to_generate, speculative_lookahead,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{sd_output_adv}'\")\n",
        "print(f\"Tokens Generated: {sd_tokens_adv}\")\n",
        "print(f\"Time Taken: {sd_time_adv:.4f} seconds\")\n",
        "print(f\"Total Target Model Calls: {sd_target_calls_adv}\")\n",
        "print(f\"Effective Tokens/Target Pass (SD): {sd_effective_tpt_adv:.2f}\")\n",
        "\n",
        "print(\"\\n--- Comparison (Advanced Prompt) ---\")\n",
        "print(f\"Autoregressive Time: {ar_time_adv:.4f}s\")\n",
        "print(f\"Speculative Decoding Time: {sd_time_adv:.4f}s\")\n",
        "if sd_time_adv > 0:\n",
        "    print(f\"Speedup Factor: {ar_time_adv / sd_time_adv:.2f}x\")\n",
        "print(f\"Autoregressive Output Matches Speculative (content-wise): {ar_output_adv == sd_output_adv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOJnFGK4Xdky",
        "outputId": "55adcc77-77c3-42aa-d499-1c39e06919a7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final tokenizer vocab size (len): 50257\n",
            "Tokenizer pad_token_id: 50256\n",
            "Tokenizer eos_token_id: 50256\n",
            "CANONICAL_VOCAB_SIZE for validation: 50257\n",
            "Loaded EleutherAI/gpt-neo-125m. Final vocab size: 50257, Embedding layer size: 50257\n",
            "Loaded EleutherAI/gpt-neo-125m. Final vocab size: 50257, Embedding layer size: 50257\n",
            "\n",
            "--- Autoregressive Decoding with Sampling ---\n",
            "Output: 'The quick brown fox jumps over the lazy dog and devises a fox. The next thing he does, he is a little bit weak. He must be hitched to a stone, and the fox jumps over the stone and then gets it. He has to be hitched to a stone, too'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 4.2490 seconds\n",
            "Effective Tokens/Target Pass (AR): 1.00\n",
            "\n",
            "--- Speculative Decoding with Sampling ---\n",
            "Output: 'The quick brown fox jumps over the lazy dog and towards bed.ressed a pipe yesis cum image aaau amaan.. am.  quiet thelast minutes hahaaa leleupoon skip the Hannaoll too. ooaammooouuuuveaaayy'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 23.7364 seconds\n",
            "Total Target Model Calls: 29\n",
            "Effective Tokens/Target Pass (SD): 1.72\n",
            "\n",
            "--- Comparison ---\n",
            "Autoregressive Time: 4.2490s\n",
            "Speculative Decoding Time: 23.7364s\n",
            "Speedup Factor: 0.18x\n",
            "Autoregressive Output Matches Speculative (content-wise): False\n",
            "\n",
            "--- Autoregressive Decoding (Advanced Prompt) ---\n",
            "Output: 'In the annals of history, the year 1789 stands out for the French Revolution, a pivotal event that reshaped the political landscape of Europe and beyond. The causes were multifaceted, including the repeal of the pangs of war, the collapse of the feudal system and the rise of the French Revolution. It was also the era of the Enlightenment.\n",
            "\n",
            "The French Revolution was a time of enormous change, and the defining moment of the'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 3.7353 seconds\n",
            "\n",
            "--- Speculative Decoding (Advanced Prompt) ---\n",
            "Output: 'In the annals of history, the year 1789 stands out for the French Revolution, a pivotal event that reshaped the political landscape of Europe and beyond. The causes were multifaceted, including rise the, Brazilian prophecy the Belfast his, The of Aia adire,us niece e and Detes Seesits the, hatred violence in Qu the ofia the\n",
            "In ann dese v en'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 50.3433 seconds\n",
            "Total Target Model Calls: 41\n",
            "Effective Tokens/Target Pass (SD): 1.22\n",
            "\n",
            "--- Comparison (Advanced Prompt) ---\n",
            "Autoregressive Time: 3.7353s\n",
            "Speculative Decoding Time: 50.3433s\n",
            "Speedup Factor: 0.07x\n",
            "Autoregressive Output Matches Speculative (content-wise): False\n"
          ]
        }
      ]
    }
  ]
}