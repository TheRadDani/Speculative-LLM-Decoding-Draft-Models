{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNiiA2LCtOfZ+FUfL5qqcwh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheRadDani/Speculative-LLM-Decoding-Draft-Models/blob/main/Speculative_Decoding_Draft_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.nn.functional as F # For softmax"
      ],
      "metadata": {
        "id": "FAoOxEkPONqA"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "target_model_name = \"EleutherAI/gpt-neo-125m\"\n",
        "draft_model_name = \"distilbert/distilgpt2\"\n",
        "\n",
        "# Load tokenizer first to determine the vocabulary size consistently\n",
        "tokenizer = AutoTokenizer.from_pretrained(target_model_name)"
      ],
      "metadata": {
        "id": "7S7XUNr8ORM0"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Canonical Vocabulary Size Handling ---\n",
        "\n",
        "CANONICAL_VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "# Check if pad_token is already defined in the tokenizer\n",
        "if tokenizer.pad_token is None:\n",
        "    # If not, add it using the EOS token as a common practice for causal LMs\n",
        "    # This will increase the tokenizer's vocabulary size by 1.\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "    CANONICAL_VOCAB_SIZE = len(tokenizer) # Update canonical size\n",
        "else:\n",
        "    # Ensure CANONICAL_VOCAB_SIZE is correct even if pad_token was already present\n",
        "    CANONICAL_VOCAB_SIZE = len(tokenizer)\n",
        "\n",
        "print(f\"Final tokenizer vocab size (len): {len(tokenizer)}\")\n",
        "print(f\"Tokenizer pad_token_id: {tokenizer.pad_token_id}\")\n",
        "print(f\"Tokenizer eos_token_id: {tokenizer.eos_token_id}\")\n",
        "print(f\"CANONICAL_VOCAB_SIZE for validation: {CANONICAL_VOCAB_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeXCwMbHOiZu",
        "outputId": "678e8fd2-7089-4020-9216-e38187e0db55"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final tokenizer vocab size (len): 50257\n",
            "Tokenizer pad_token_id: 50256\n",
            "Tokenizer eos_token_id: 50256\n",
            "CANONICAL_VOCAB_SIZE for validation: 50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load both models using the helper function\n",
        "target_model = load_and_resize_model(target_model_name, CANONICAL_VOCAB_SIZE)\n",
        "draft_model = load_and_resize_model(draft_model_name, CANONICAL_VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4a16JfHOqUs",
        "outputId": "b6ea08bd-a09b-49ae-d127-1354a24aeeda"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded EleutherAI/gpt-neo-125m. Final vocab size: 50257, Embedding layer size: 50257\n",
            "Loaded distilbert/distilgpt2. Final vocab size: 50257, Embedding layer size: 50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to load model and adjust its embedding layer size\n",
        "def load_and_resize_model(model_name, canonical_vocab_size):\n",
        "    \"\"\"\n",
        "    Loads a causal language model and ensures its embedding layer matches\n",
        "    the canonical vocabulary size.\n",
        "    \"\"\"\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "    # Important: Adjust vocab_size in config before loading, if it's smaller.\n",
        "    # This guides the model's initialization of its embedding layer.\n",
        "    if config.vocab_size < canonical_vocab_size:\n",
        "        print(f\"Adjusting {model_name} config vocab_size from {config.vocab_size} to {canonical_vocab_size}\")\n",
        "        config.vocab_size = canonical_vocab_size\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
        "\n",
        "    # After loading, explicitly resize embeddings if they still don't match.\n",
        "    # This can happen if the loaded model's weights don't perfectly align with the config changes.\n",
        "    if model.get_input_embeddings().num_embeddings < canonical_vocab_size:\n",
        "        print(f\"Resizing {model_name} embeddings from {model.get_input_embeddings().num_embeddings} to {canonical_vocab_size}\")\n",
        "        model.resize_token_embeddings(canonical_vocab_size)\n",
        "\n",
        "    model.to(device)\n",
        "    print(f\"Loaded {model_name}. Final vocab size: {model.config.vocab_size}, Embedding layer size: {model.get_input_embeddings().num_embeddings}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "2jNtPOTeOnc0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for robust sampling\n",
        "def sample_next_token(logits, temperature=1.0, top_k=0, top_p=1.0, model_vocab_size=None, tokenizer_ref=None):\n",
        "    \"\"\"\n",
        "    Samples the next token from the logits, with optional temperature, Top-K, and Top-P sampling.\n",
        "    Includes robust validation for sampled token IDs.\n",
        "    \"\"\"\n",
        "    if temperature == 0.0: # Greedy decoding\n",
        "        next_token_id = torch.argmax(logits, dim=-1).item()\n",
        "    else:\n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Top-K sampling\n",
        "        if top_k > 0:\n",
        "            top_k_actual = min(top_k, logits.size(-1))\n",
        "            values, _ = torch.topk(logits, top_k_actual)\n",
        "            min_value = values[:, -1].unsqueeze(-1)\n",
        "            logits = torch.where(logits < min_value, torch.full_like(logits, -float('Inf')), logits)\n",
        "\n",
        "        # Top-P (nucleus) sampling\n",
        "        if top_p < 1.0:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            # Shift the indices to the right to keep the first token above the threshold\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "            # Set logits of removed tokens to -Inf\n",
        "            logits = logits.scatter_(-1, sorted_indices[sorted_indices_to_remove], float('-Inf'))\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Handle cases where probabilities might become all zero after aggressive filtering\n",
        "        probabilities = torch.nan_to_num(probabilities, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        if probabilities.sum().item() == 0.0:\n",
        "            print(\"Warning: All probabilities zero after filtering. Falling back to EOS.\")\n",
        "            # Ensure tokenizer_ref is provided for this fallback\n",
        "            if tokenizer_ref is not None and tokenizer_ref.eos_token_id is not None:\n",
        "                return tokenizer_ref.eos_token_id\n",
        "            else:\n",
        "                # If no EOS token, return a safe default like 0 (first token)\n",
        "                return 0\n",
        "\n",
        "        # Sample from the (possibly filtered) distribution\n",
        "        next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
        "\n",
        "    # --- Robust validation for sampled token ID ---\n",
        "    if model_vocab_size is not None:\n",
        "        if not (0 <= next_token_id < model_vocab_size):\n",
        "            print(f\"!!! CRITICAL ERROR: Sampled token ID {next_token_id} is out of vocabulary range [{0}, {model_vocab_size-1}]\")\n",
        "            print(f\"Logits shape: {logits.shape}\")\n",
        "            print(f\"Probabilities sum: {probabilities.sum().item():.4f}\")\n",
        "            top_probs, top_indices = torch.topk(probabilities, k=min(10, probabilities.size(-1)))\n",
        "            print(f\"Top 10 Probs: {top_probs.tolist()}\")\n",
        "            print(f\"Top 10 Indices: {top_indices.tolist()}\")\n",
        "            raise IndexError(\"Sampled token ID out of model's vocabulary range.\")\n",
        "\n",
        "    return next_token_id"
      ],
      "metadata": {
        "id": "uiRTHuxeO66T"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoregressive_decode_with_sampling(prompt: str, target_model, tokenizer,\n",
        "                                         max_new_tokens: int = 50,\n",
        "                                         temperature: float = 1.0,\n",
        "                                         top_k: int = 0,\n",
        "                                         top_p: float = 1.0):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = []\n",
        "    past_key_values = None\n",
        "\n",
        "    start_time = time.time()\n",
        "    for _ in range(max_new_tokens):\n",
        "        # --- Validate input_ids before passing to model ---\n",
        "        if input_ids.numel() > 0 and (input_ids.min() < 0 or input_ids.max() >= CANONICAL_VOCAB_SIZE):\n",
        "            print(f\"!!! ERROR: AR input_ids min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
        "            print(f\"AR input_ids: {input_ids}\")\n",
        "            raise IndexError(\"AR input_ids contain out-of-range token for target model.\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = target_model(input_ids, past_key_values=past_key_values, use_cache=True)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            past_key_values = outputs.past_key_values\n",
        "\n",
        "            next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                                             model_vocab_size=CANONICAL_VOCAB_SIZE, tokenizer_ref=tokenizer)\n",
        "\n",
        "            generated_tokens.append(next_token_id)\n",
        "            input_ids = torch.tensor([[next_token_id]]).to(device)\n",
        "\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    end_time = time.time()\n",
        "    full_sequence = tokenizer.decode(tokenizer.encode(prompt) + generated_tokens, skip_special_tokens=True)\n",
        "    return full_sequence, len(generated_tokens), (end_time - start_time)"
      ],
      "metadata": {
        "id": "pTJv10lTO-Cu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def speculative_decode(prompt: str, target_model, draft_model, tokenizer,\n",
        "                       max_new_tokens: int = 50, speculative_lookahead: int = 5,\n",
        "                       temperature: float = 1.0, top_k: int = 0, top_p: float = 1.0):\n",
        "    input_ids_initial_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = []\n",
        "\n",
        "    target_past_key_values = None\n",
        "    draft_past_key_values = None\n",
        "\n",
        "    current_validated_prefix_ids = input_ids_initial_prompt\n",
        "\n",
        "    n_accepted_tokens_total = 0\n",
        "    n_target_model_calls_total = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while len(generated_tokens) < max_new_tokens:\n",
        "        initial_cycle_prefix_length = current_validated_prefix_ids.shape[1]\n",
        "\n",
        "        # Draft Phase: Generate speculative tokens ---\n",
        "        # Draft model's KV cache is recomputed from scratch based on the current_validated_prefix_ids\n",
        "        # for each speculative cycle to avoid any potential misalignment.\n",
        "        temp_draft_input_ids = current_validated_prefix_ids.clone()\n",
        "        current_draft_past_key_values = None # Force recompute draft KV cache for this cycle\n",
        "\n",
        "        draft_proposed_tokens = []\n",
        "        draft_logits_history = []\n",
        "\n",
        "        for i in range(speculative_lookahead):\n",
        "            if temp_draft_input_ids.numel() == 0:\n",
        "                break\n",
        "\n",
        "            if temp_draft_input_ids.min() < 0 or temp_draft_input_ids.max() >= CANONICAL_VOCAB_SIZE:\n",
        "                print(f\"!!! ERROR: Draft model input_ids contain out-of-range token prior to generation step {i}.\")\n",
        "                print(f\"Min ID: {temp_draft_input_ids.min().item()}, Max ID: {temp_draft_input_ids.max().item()}\")\n",
        "                print(f\"Draft model vocab size: {CANONICAL_VOCAB_SIZE}\")\n",
        "                print(f\"Offending input_ids: {temp_draft_input_ids}\")\n",
        "                raise IndexError(\"Draft model input_ids out of vocabulary range.\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                draft_outputs = draft_model(temp_draft_input_ids, past_key_values=current_draft_past_key_values, use_cache=True)\n",
        "                draft_logits = draft_outputs.logits[:, -1, :]\n",
        "                current_draft_past_key_values = draft_outputs.past_key_values # Update draft's KV cache for next proposal\n",
        "\n",
        "                draft_probs = torch.softmax(draft_logits, dim=-1)\n",
        "                next_draft_token = torch.multinomial(draft_probs, num_samples=1).item()\n",
        "\n",
        "                if not (0 <= next_draft_token < CANONICAL_VOCAB_SIZE):\n",
        "                    print(f\"!!! ERROR: Draft model generated out-of-vocab token: {next_draft_token}\")\n",
        "                    print(f\"Draft model vocab size: {CANONICAL_VOCAB_SIZE}\")\n",
        "                    raise IndexError(\"Draft model generated token ID out of range.\")\n",
        "\n",
        "                draft_proposed_tokens.append(next_draft_token)\n",
        "                draft_logits_history.append(draft_logits)\n",
        "\n",
        "                temp_draft_input_ids = torch.tensor([[next_draft_token]]).to(device)\n",
        "\n",
        "                if next_draft_token == tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        if not draft_proposed_tokens:\n",
        "            print(\"Draft model proposed no tokens or hit EOS immediately. Falling back to single target model generation.\")\n",
        "            if current_validated_prefix_ids.numel() == 0:\n",
        "                print(\"Warning: current_validated_prefix_ids is empty in fallback, cannot generate.\")\n",
        "                break\n",
        "\n",
        "            if current_validated_prefix_ids.min() < 0 or current_validated_prefix_ids.max() >= CANONICAL_VOCAB_SIZE:\n",
        "                print(f\"!!! ERROR: Fallback input_ids min: {current_validated_prefix_ids.min().item()}, max: {current_validated_prefix_ids.max().item()}\")\n",
        "                print(f\"Fallback input_ids: {current_validated_prefix_ids}\")\n",
        "                raise IndexError(\"Fallback input_ids contain out-of-range token for target model.\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = target_model(current_validated_prefix_ids, past_key_values=target_past_key_values, use_cache=True)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                target_past_key_values = outputs.past_key_values\n",
        "\n",
        "                next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                                                  model_vocab_size=CANONICAL_VOCAB_SIZE, tokenizer_ref=tokenizer)\n",
        "\n",
        "                generated_tokens.append(next_token_id)\n",
        "                current_validated_prefix_ids = torch.cat((current_validated_prefix_ids, torch.tensor([[next_token_id]]).to(device)), dim=1)\n",
        "                n_target_model_calls_total += 1\n",
        "\n",
        "                # --- Aggressive KV cache reset after fallback single-token generation ---\n",
        "                # This ensures the KV cache is precisely aligned with the *current_validated_prefix_ids*\n",
        "                # for the start of the *next* speculative cycle.\n",
        "                target_past_key_values = None\n",
        "\n",
        "                if next_token_id == tokenizer.eos_token_id:\n",
        "                    break\n",
        "            continue\n",
        "\n",
        "        # Verification Phase: Parallel evaluation by the target model ---\n",
        "        full_eval_input_ids = torch.cat((current_validated_prefix_ids, torch.tensor([draft_proposed_tokens]).to(device)), dim=1)\n",
        "\n",
        "        if full_eval_input_ids.numel() > 0 and \\\n",
        "           (full_eval_input_ids.min() < 0 or full_eval_input_ids.max() >= CANONICAL_VOCAB_SIZE):\n",
        "            print(f\"!!! ERROR: Combined input_ids for target model contain out-of-range token.\")\n",
        "            print(f\"Min ID: {full_eval_input_ids.min().item()}, Max ID: {full_eval_input_ids.max().item()}\")\n",
        "            print(f\"Target model vocab size: {CANONICAL_VOCAB_SIZE}\")\n",
        "            offending_tokens = full_eval_input_ids[ (full_eval_input_ids < 0) | (full_eval_input_ids >= CANONICAL_VOCAB_SIZE) ]\n",
        "            print(f\"Offending tokens: {offending_tokens}\")\n",
        "            print(f\"Full eval input IDs: {full_eval_input_ids}\")\n",
        "            raise IndexError(\"Full eval input_ids for target model out of vocabulary range.\")\n",
        "\n",
        "        n_target_model_calls_total += 1\n",
        "        with torch.no_grad():\n",
        "            target_outputs = target_model(full_eval_input_ids, past_key_values=target_past_key_values, use_cache=True)\n",
        "            target_logits_full_sequence = target_outputs.logits\n",
        "            # Store the KV cache from this full evaluation for potential slicing upon rejection\n",
        "            target_past_key_values_from_full_eval = target_outputs.past_key_values\n",
        "\n",
        "        # Rejection Sampling ---\n",
        "        accepted_count_in_this_cycle = 0\n",
        "\n",
        "        target_logits_start_idx_in_full_sequence = current_validated_prefix_ids.shape[1]\n",
        "\n",
        "        for i, draft_token_id in enumerate(draft_proposed_tokens):\n",
        "            target_logits_at_idx = target_logits_full_sequence[:, target_logits_start_idx_in_full_sequence + i, :]\n",
        "            target_prob_for_draft = torch.softmax(target_logits_at_idx, dim=-1)[:, draft_token_id].item()\n",
        "\n",
        "            draft_logits_at_idx = draft_logits_history[i]\n",
        "            draft_prob_for_draft = torch.softmax(draft_logits_at_idx, dim=-1)[:, draft_token_id].item()\n",
        "\n",
        "            acceptance_prob = min(1.0, target_prob_for_draft / (draft_prob_for_draft + 1e-9))\n",
        "\n",
        "            u = np.random.rand()\n",
        "            if u <= acceptance_prob:\n",
        "                generated_tokens.append(draft_token_id)\n",
        "                accepted_count_in_this_cycle += 1\n",
        "                n_accepted_tokens_total += 1\n",
        "                current_validated_prefix_ids = torch.cat((current_validated_prefix_ids, torch.tensor([[draft_token_id]]).to(device)), dim=1)\n",
        "\n",
        "                if draft_token_id == tokenizer.eos_token_id:\n",
        "                    # If an accepted token is EOS, we stop immediately.\n",
        "                    break\n",
        "            else:\n",
        "                # Rejection: Sample from the target model's distribution for the current token\n",
        "                rejection_logits = target_logits_at_idx\n",
        "                next_token_id_after_rejection = sample_next_token(\n",
        "                    rejection_logits,\n",
        "                    temperature=temperature,\n",
        "                    top_k=top_k,\n",
        "                    top_p=top_p,\n",
        "                    model_vocab_size=CANONICAL_VOCAB_SIZE,\n",
        "                    tokenizer_ref=tokenizer\n",
        "                )\n",
        "\n",
        "                generated_tokens.append(next_token_id_after_rejection)\n",
        "                n_accepted_tokens_total += 1\n",
        "                current_validated_prefix_ids = torch.cat((current_validated_prefix_ids, torch.tensor([[next_token_id_after_rejection]]).to(device)), dim=1)\n",
        "\n",
        "                # --- Aggressive KV cache reset after rejection ---\n",
        "                # This is CRUCIAL. It means the KV cache is discarded and recreated from scratch\n",
        "                # in the *next* speculative cycle, using the newly validated `current_validated_prefix_ids`.\n",
        "                # This eliminates any potential for misaligned cached states.\n",
        "                target_past_key_values = None\n",
        "\n",
        "                if next_token_id_after_rejection == tokenizer.eos_token_id:\n",
        "                    break\n",
        "                break # Break from inner 'for' loop after rejection\n",
        "\n",
        "        # Check if max_new_tokens reached or EOS generated\n",
        "        if len(generated_tokens) >= max_new_tokens or (generated_tokens and generated_tokens[-1] == tokenizer.eos_token_id):\n",
        "            break\n",
        "\n",
        "        # --- Final fallback to ensure progress (if no tokens were accepted and not EOS) ---\n",
        "        # This ensures we don't get stuck if the draft model is very bad and no accepted tokens.\n",
        "        # Check if current_validated_prefix_ids hasn't grown since the start of this cycle,\n",
        "        # implying nothing was accepted from the speculative batch.\n",
        "        if current_validated_prefix_ids.shape[1] == initial_cycle_prefix_length and len(generated_tokens) < max_new_tokens:\n",
        "            print(\"Warning: No tokens accepted in speculative step, and no rejection fallback occurred. Performing single target model step to ensure progress.\")\n",
        "            if current_validated_prefix_ids.numel() == 0:\n",
        "                print(\"Warning: current_validated_prefix_ids is empty in final fallback. Cannot generate.\")\n",
        "                break\n",
        "\n",
        "            if current_validated_prefix_ids.min() < 0 or current_validated_prefix_ids.max() >= CANONICAL_VOCAB_SIZE:\n",
        "                print(f\"!!! ERROR: Final fallback input_ids min: {current_validated_prefix_ids.min().item()}, max: {current_validated_prefix_ids.max().item()}\")\n",
        "                print(f\"Final fallback input_ids: {current_validated_prefix_ids}\")\n",
        "                raise IndexError(\"Final fallback input_ids contain out-of-range token for target model.\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = target_model(current_validated_prefix_ids, past_key_values=target_past_key_values, use_cache=True)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                target_past_key_values = outputs.past_key_values\n",
        "\n",
        "                next_token_id = sample_next_token(logits, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                                                   model_vocab_size=CANONICAL_VOCAB_SIZE, tokenizer_ref=tokenizer)\n",
        "\n",
        "                generated_tokens.append(next_token_id)\n",
        "                current_validated_prefix_ids = torch.cat((current_validated_prefix_ids, torch.tensor([[next_token_id]]).to(device)), dim=1)\n",
        "                n_target_model_calls_total += 1\n",
        "\n",
        "                # --- Aggressive KV cache reset after final fallback ---\n",
        "                target_past_key_values = None\n",
        "\n",
        "                if next_token_id == tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    # Decode the final sequence\n",
        "    full_sequence_ids = tokenizer.encode(prompt) + generated_tokens\n",
        "    full_sequence = tokenizer.decode(full_sequence_ids, skip_special_tokens=True)\n",
        "\n",
        "    tokens_generated = len(generated_tokens)\n",
        "    effective_tokens_per_target_pass = tokens_generated / n_target_model_calls_total if n_target_model_calls_total > 0 else 0\n",
        "\n",
        "    return full_sequence, tokens_generated, (end_time - start_time), effective_tokens_per_target_pass, n_target_model_calls_total"
      ],
      "metadata": {
        "id": "SOJnFGK4Xdky"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Demonstration ---\n",
        "prompt = \"The quick brown fox jumps over the lazy dog and\"\n",
        "max_tokens_to_generate = 50\n",
        "speculative_lookahead = 5\n",
        "\n",
        "sampling_temperature = 0.7\n",
        "sampling_top_k = 0\n",
        "sampling_top_p = 1.0\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding with Sampling ---\")\n",
        "ar_output, ar_tokens, ar_time = autoregressive_decode_with_sampling(\n",
        "    prompt, target_model, tokenizer, max_tokens_to_generate,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{ar_output}'\")\n",
        "print(f\"Tokens Generated: {ar_tokens}\")\n",
        "print(f\"Time Taken: {ar_time:.4f} seconds\")\n",
        "print(f\"Effective Tokens/Target Pass (AR): {ar_tokens / ar_tokens:.2f}\" if ar_tokens > 0 else \"N/A\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Speculative Decoding with Sampling ---\")\n",
        "sd_output, sd_tokens, sd_time, sd_effective_tpt, sd_target_calls = speculative_decode(\n",
        "    prompt, target_model, draft_model, tokenizer, max_tokens_to_generate, speculative_lookahead,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{sd_output}'\")\n",
        "print(f\"Tokens Generated: {sd_tokens}\")\n",
        "print(f\"Time Taken: {sd_time:.4f} seconds\")\n",
        "print(f\"Total Target Model Calls: {sd_target_calls}\")\n",
        "print(f\"Effective Tokens/Target Pass (SD): {sd_effective_tpt:.2f}\")\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Autoregressive Time: {ar_time:.4f}s\")\n",
        "print(f\"Speculative Decoding Time: {sd_time:.4f}s\")\n",
        "if sd_time > 0:\n",
        "    print(f\"Speedup Factor: {ar_time / sd_time:.2f}x\")\n",
        "# For sampling, outputs are not guaranteed to be identical due to randomness.\n",
        "# The guarantee is on the *distribution* of outputs matching.\n",
        "print(f\"Autoregressive Output Matches Speculative (content-wise): {ar_output == sd_output}\")\n",
        "\n",
        "\n",
        "prompt_advanced = \"In the annals of history, the year 1789 stands out for the French Revolution, a pivotal event that reshaped the political landscape of Europe and beyond. The causes were multifaceted, including\"\n",
        "\n",
        "print(\"\\n--- Autoregressive Decoding (Advanced Prompt) ---\")\n",
        "ar_output_adv, ar_tokens_adv, ar_time_adv = autoregressive_decode_with_sampling(\n",
        "    prompt_advanced, target_model, tokenizer, max_tokens_to_generate,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{ar_output_adv}'\")\n",
        "print(f\"Tokens Generated: {ar_tokens_adv}\")\n",
        "print(f\"Time Taken: {ar_time_adv:.4f} seconds\")\n",
        "\n",
        "print(\"\\n--- Speculative Decoding (Advanced Prompt) ---\")\n",
        "sd_output_adv, sd_tokens_adv, sd_time_adv, sd_effective_tpt_adv, sd_target_calls_adv = speculative_decode(\n",
        "    prompt_advanced, target_model, draft_model, tokenizer, max_tokens_to_generate, speculative_lookahead,\n",
        "    temperature=sampling_temperature, top_k=sampling_top_k, top_p=sampling_top_p\n",
        ")\n",
        "print(f\"Output: '{sd_output_adv}'\")\n",
        "print(f\"Tokens Generated: {sd_tokens_adv}\")\n",
        "print(f\"Time Taken: {sd_time_adv:.4f} seconds\")\n",
        "print(f\"Total Target Model Calls: {sd_target_calls_adv}\")\n",
        "print(f\"Effective Tokens/Target Pass (SD): {sd_effective_tpt_adv:.2f}\")\n",
        "\n",
        "print(\"\\n--- Comparison (Advanced Prompt) ---\")\n",
        "print(f\"Autoregressive Time: {ar_time_adv:.4f}s\")\n",
        "print(f\"Speculative Decoding Time: {sd_time_adv:.4f}s\")\n",
        "if sd_time_adv > 0:\n",
        "    print(f\"Speedup Factor: {ar_time_adv / sd_time_adv:.2f}x\")\n",
        "print(f\"Autoregressive Output Matches Speculative (content-wise): {ar_output_adv == sd_output_adv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx0pdMrxPan-",
        "outputId": "481b2368-56c0-474f-f6a5-d896c7543a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Autoregressive Decoding with Sampling ---\n",
            "Output: 'The quick brown fox jumps over the lazy dog and goes for a walk, the dog in the park, the park and the dog pulling the leash. They are both the lovers of the horse.\n",
            "\n",
            "They have been out for about a week and they haven't Hardcore. They are here to stay'\n",
            "Tokens Generated: 50\n",
            "Time Taken: 9.8377 seconds\n",
            "Effective Tokens/Target Pass (AR): 1.00\n",
            "\n",
            "--- Speculative Decoding with Sampling ---\n"
          ]
        }
      ]
    }
  ]
}